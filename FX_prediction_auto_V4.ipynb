{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V4は自動でデータの組み合わせを変更し、自動で入力系列数と出力系列数を変更するプログラムです。ただし、未完成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils, plot_model\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.initializers import glorot_uniform, orthogonal, TruncatedNormal\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.recurrent import GRU, SimpleRNN\n",
    "\n",
    "import pandas as pd #行列計算\n",
    "import numpy as np #行列計算\n",
    "import math #数値計算\n",
    "import itertools #順列・組み合わせ\n",
    "import time\n",
    "import talib as ta # テクニカル指標\n",
    "\n",
    "import matplotlib.pyplot as plt #グラフ\n",
    "import winsound # ビープ音\n",
    "import os\n",
    "\n",
    "# マルチプロセス\n",
    "from multiprocessing import Process\n",
    "from multiprocessing import Queue\n",
    "\n",
    "from df_method import rise_fall_rate, moving_average, GCDC, df_shift, add_data, RSI, Z_score_normalization, Min_Max_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction :\n",
    "    #初期化\n",
    "    def __init__(self, maxlen, n_hidden, n_in, n_out, learning_model):\n",
    "        self.maxlen = maxlen #入力系列数\n",
    "        self.n_hidden = n_hidden #出力次元（隠れ層内のニューロン数）\n",
    "        self.n_in = n_in #学習データの列数\n",
    "        self.n_out = n_out #ラベルデータの列数\n",
    "        \n",
    "        self.learning_model = learning_model #●学習モデルの選択\n",
    "\n",
    "    #モデルの生成\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        if self.learning_model == 'RNN':\n",
    "            #RNN層\n",
    "            model.add(SimpleRNN(self.n_hidden,\n",
    "                                batch_input_shape = (None, self.maxlen, self.n_in),\n",
    "                                kernel_initializer = glorot_uniform(seed=20170719),\n",
    "                                recurrent_initializer = orthogonal(gain=1.0, seed=20170719),\n",
    "                                dropout = 0.5,\n",
    "                                recurrent_dropout = 0.5))\n",
    "        elif self.learning_model == 'LSTM':\n",
    "            #LSTM層\n",
    "            model.add(LSTM(self.n_hidden,\n",
    "                           batch_input_shape = (None, self.maxlen, self.n_in),\n",
    "                           kernel_initializer = glorot_uniform(seed=20170719), \n",
    "                           recurrent_initializer = orthogonal(gain=1.0, seed=20170719), \n",
    "                           dropout = 0.5, \n",
    "                           recurrent_dropout = 0.5))\n",
    "        elif self.learning_model == 'GRU':\n",
    "            #GRU層\n",
    "            model.add(GRU(self.n_hidden,\n",
    "                          batch_input_shape = (None, self.maxlen, self.n_in),\n",
    "                          kernel_initializer = glorot_uniform(seed=20170719),\n",
    "                          recurrent_initializer = orthogonal(gain=1.0, seed=20170719),\n",
    "                          dropout = 0.5,\n",
    "                          recurrent_dropout = 0.5))\n",
    "        #ドロップアウト層\n",
    "        model.add(Dropout(0.5))\n",
    "        #結合層\n",
    "        model.add(Dense(self.n_out, kernel_initializer = glorot_uniform(seed=20170719)))\n",
    "        #活性化層\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        #コンパイル\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer = \"Adam\", metrics = ['categorical_accuracy']) # \"RMSprop\"\n",
    "        return model\n",
    "\n",
    "    # 学習\n",
    "    def train(self, x_train, t_train, batch_size, epochs) :\n",
    "        early_stopping = EarlyStopping(patience=0, verbose=1)\n",
    "        model = self.create_model()\n",
    "        self.hist = model.fit(x_train, t_train, batch_size = batch_size, epochs = epochs, verbose = 1,\n",
    "                              shuffle = True, callbacks = [early_stopping], validation_split = 0.1)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 変数宣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_debug = True\n",
    "csv_path = './csv/'\n",
    "x_days_later = 1 # '1', '2', '7', '30', '365'\n",
    "learning_model = 'GRU' #['RNN', 'LSTM', 'GRU']\n",
    "start_date = '2009-3-10' #  '1960', '1970', '1980', '1990', '2000', '2010'\n",
    "end_date = '2018-10-31'# 終点年月日\n",
    "\n",
    "min_maxlen = 100\n",
    "max_maxlen = 1000\n",
    "min_n_hidden = 100\n",
    "max_n_hidden = 500\n",
    "\n",
    "target_name = 'USD_JPY_diff'# 'nikkei_Close', 'nikkei_diff', 'USD_JPY'\n",
    "max_score = 0 # 最高正答率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データの用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = {} # データフレームの辞書\n",
    "df_list_name = ''\n",
    "\n",
    "#ドル円の始値と終値の差\n",
    "df = pd.read_csv(csv_path+'USD_JPY.csv', index_col='Date', parse_dates=True)#読み込み\n",
    "df = df.apply(np.log)*100#正規化\n",
    "df = df['USD_JPY_Close'] - df['USD_JPY_Open']#終値と始値の差を求める\n",
    "df = df['1989-10-16':]#始値と終値と高値と安値が記録され始めた日からのみ抽出\n",
    "df = df.rename('USD_JPY_diff')#名前を付ける\n",
    "df = pd.DataFrame(df)# sriesからdataframeに変換\n",
    "df_list['USD_JPY_diff'] = df\n",
    "\n",
    "# 為替レート\n",
    "df_list['USD_JPY'] = add_data(csv_path+'USD_JPY.csv', x_days_later)\n",
    "#df_list['EUR_JPY'] = add_data(csv_path+'EUR_JPY.csv', x_days_later)\n",
    "#df_list['EUR_USD'] = add_data(csv_path+'EUR_USD.csv', x_days_later)\n",
    "\n",
    "# 株価指数\n",
    "#df_list['nikkei'] = add_data(csv_path+'nikkei.csv', x_days_later)\n",
    "#df_list['TOPIX'] = add_data(csv_path+'TOPIX.csv', x_days_later)\n",
    "#df_list['DOW30'] = add_data(csv_path+'DOW30.csv', x_days_later)\n",
    "#df_list['NASDAQ'] = add_data(csv_path+'NASDAQ.csv', x_days_later)\n",
    "\n",
    "# 米国債\n",
    "df = pd.read_csv(csv_path+'treasury_10.csv', index_col='Date', parse_dates=True)\n",
    "df_list['treasury_10'] = rise_fall_rate(df, x_days_later)\n",
    "\n",
    "# テクニカル指標を求めるために使用-----------------------------------------\n",
    "df = pd.read_csv(csv_path+'USD_JPY.csv', index_col=0, parse_dates=True)\n",
    "df_close = df['USD_JPY_Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全データフレームを結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            USD_JPY_diff  USD_JPY_Open  USD_JPY_High  USD_JPY_Low  \\\n",
      "Date                                                                \n",
      "2009-03-10     -0.182279      0.578359     -0.040339     0.010213   \n",
      "2009-03-11     -1.418753     -0.172143     -0.303061    -0.882147   \n",
      "2009-03-12      0.451283     -1.418753     -0.324281    -1.442457   \n",
      "2009-03-13      0.326931      0.451283      0.131866     1.524847   \n",
      "2009-03-16      0.091710      0.377918      0.000000     0.861014   \n",
      "2009-03-17      0.416731      0.091710      0.323854     0.112205   \n",
      "2009-03-18     -2.433260      0.416731     -0.131439    -2.477162   \n",
      "2009-03-19     -1.792595     -2.402086     -2.282013    -2.261775   \n",
      "2009-03-20      1.481042     -1.834914     -0.352551     0.649941   \n",
      "2009-03-23      1.297843      1.251397      1.105326     1.580478   \n",
      "2009-03-24      0.944469      1.308158      1.235403     1.318883   \n",
      "2009-03-25     -0.378653      0.975117     -0.203149     0.010316   \n",
      "2009-03-26      1.212754     -0.378653      0.527331     0.514457   \n",
      "2009-03-27     -0.803544      1.202624      0.000000    -0.339245   \n",
      "2009-03-30     -0.961939     -0.528188     -0.557839    -1.180871   \n",
      "2009-03-31      1.753179     -0.961939      1.042150     1.304366   \n",
      "2009-04-01     -0.415211      1.732969      0.110658     0.992691   \n",
      "2009-04-02      0.969509     -0.415211      0.431403     0.203459   \n",
      "2009-04-03      0.740967      0.969509      0.449484     0.940684   \n",
      "2009-04-06      0.486571      0.960199      1.080449     1.011678   \n",
      "2009-04-07     -0.536461      0.466757     -0.335770    -0.489536   \n",
      "2009-04-08     -0.649580     -0.536461     -0.247611    -0.562420   \n",
      "2009-04-09      0.659607     -0.659607     -0.297944     0.341846   \n",
      "2009-04-10     -0.079721      0.659607      0.168944     0.580466   \n",
      "2009-04-13     -0.289263      0.009961     -0.019861    -0.359892   \n",
      "2009-04-14     -1.155269     -0.279274     -0.278469    -1.117893   \n",
      "2009-04-15      0.363087     -1.155269     -0.759776    -0.619700   \n",
      "2009-04-16     -0.070497      0.363087     -0.130542     0.386494   \n",
      "2009-04-17     -0.050385     -0.070497      0.210791     0.192688   \n",
      "2009-04-20     -1.298588     -0.050385     -0.361628    -1.079770   \n",
      "...                  ...           ...           ...          ...   \n",
      "2018-09-20      0.186874     -0.062330      0.115581    -0.107047   \n",
      "2018-09-21      0.097773      0.160199      0.257355     0.320799   \n",
      "2018-09-24      0.213012      0.088889     -0.035458     0.026687   \n",
      "2018-09-25      0.177211      0.186410      0.150609     0.266477   \n",
      "2018-09-26     -0.194984      0.159504      0.115019    -0.097626   \n",
      "2018-09-27      0.654754     -0.257104      0.317825    -0.044407   \n",
      "2018-09-28      0.281864      0.637114      0.211323     0.593421   \n",
      "2018-10-01      0.237206      0.281864      0.289843     0.396599   \n",
      "2018-10-02     -0.246024      0.228431     -0.008771    -0.140833   \n",
      "2018-10-03      0.753861     -0.263621      0.437599    -0.026428   \n",
      "2018-10-04     -0.472938      0.701450      0.000000     0.105671   \n",
      "2018-10-05     -0.140622     -0.516842     -0.393753    -0.044016   \n",
      "2018-10-08     -0.414262     -0.149418     -0.131596    -0.662576   \n",
      "2018-10-09     -0.221112     -0.440762     -0.484007     0.044309   \n",
      "2018-10-10     -0.612655     -0.194553     -0.105914    -0.524096   \n",
      "2018-10-11     -0.106952     -0.630469     -0.664513    -0.410532   \n",
      "2018-10-12      0.071339     -0.142628     -0.008890     0.044705   \n",
      "2018-10-15     -0.366186      0.062425     -0.258157    -0.223724   \n",
      "2018-10-16      0.419662     -0.366186      0.089095     0.107450   \n",
      "2018-10-17      0.329079      0.437481      0.311208     0.223484   \n",
      "2018-10-18     -0.409253      0.337958      0.044379    -0.071460   \n",
      "2018-10-19      0.329343     -0.418169     -0.079897     0.169635   \n",
      "2018-10-22      0.213068      0.320456      0.212917     0.204964   \n",
      "2018-10-23     -0.319801      0.204199     -0.035455    -0.338862   \n",
      "2018-10-24     -0.169197     -0.319801     -0.088692     0.133899   \n",
      "2018-10-25      0.169227     -0.187024     -0.062131    -0.250089   \n",
      "2018-10-26     -0.454810      0.187024     -0.204417    -0.394266   \n",
      "2018-10-29      0.410348     -0.463748      0.097817     0.358488   \n",
      "2018-10-30      0.621230      0.410348      0.461116     0.455216   \n",
      "2018-10-31     -0.035395      0.621230      0.282711     0.444287   \n",
      "\n",
      "            USD_JPY_Close  \n",
      "Date                       \n",
      "2009-03-10      -0.182279  \n",
      "2009-03-11      -1.408618  \n",
      "2009-03-12       0.451283  \n",
      "2009-03-13       0.326931  \n",
      "2009-03-16       0.142697  \n",
      "2009-03-17       0.416731  \n",
      "2009-03-18      -2.433260  \n",
      "2009-03-19      -1.761421  \n",
      "2009-03-20       1.438723  \n",
      "2009-03-23       1.068198  \n",
      "2009-03-24       0.954784  \n",
      "2009-03-25      -0.348004  \n",
      "2009-03-26       1.212754  \n",
      "2009-03-27      -0.813674  \n",
      "2009-03-30      -0.686584  \n",
      "2009-03-31       1.753179  \n",
      "2009-04-01      -0.435422  \n",
      "2009-04-02       0.969509  \n",
      "2009-04-03       0.740967  \n",
      "2009-04-06       0.705803  \n",
      "2009-04-07      -0.556274  \n",
      "2009-04-08      -0.649580  \n",
      "2009-04-09       0.649580  \n",
      "2009-04-10      -0.079721  \n",
      "2009-04-13      -0.199581  \n",
      "2009-04-14      -1.145281  \n",
      "2009-04-15       0.363087  \n",
      "2009-04-16      -0.070497  \n",
      "2009-04-17      -0.050385  \n",
      "2009-04-20      -1.298588  \n",
      "...                   ...  \n",
      "2018-09-20       0.177968  \n",
      "2018-09-21       0.071098  \n",
      "2018-09-24       0.204127  \n",
      "2018-09-25       0.150609  \n",
      "2018-09-26      -0.212691  \n",
      "2018-09-27       0.592634  \n",
      "2018-09-28       0.264224  \n",
      "2018-10-01       0.237206  \n",
      "2018-10-02      -0.254800  \n",
      "2018-10-03       0.736264  \n",
      "2018-10-04      -0.525349  \n",
      "2018-10-05      -0.184526  \n",
      "2018-10-08      -0.423057  \n",
      "2018-10-09      -0.247612  \n",
      "2018-10-10      -0.586095  \n",
      "2018-10-11      -0.124766  \n",
      "2018-10-12       0.035663  \n",
      "2018-10-15      -0.375101  \n",
      "2018-10-16       0.419662  \n",
      "2018-10-17       0.346898  \n",
      "2018-10-18      -0.400374  \n",
      "2018-10-19       0.320428  \n",
      "2018-10-22       0.204181  \n",
      "2018-10-23      -0.328670  \n",
      "2018-10-24      -0.169197  \n",
      "2018-10-25       0.151400  \n",
      "2018-10-26      -0.437013  \n",
      "2018-10-29       0.401410  \n",
      "2018-10-30       0.621230  \n",
      "2018-10-31      -0.035395  \n",
      "\n",
      "[2509 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#全データフレームを結合\n",
    "df_x = df_list[target_name] # 予測したいデータ\n",
    "df_list_name = target_name # 予測したいデータの名前\n",
    "for i, j in df_list.items(): #range(len(df_list) - 1):   \n",
    "    if i == target_name:\n",
    "        continue\n",
    "    df_list_name = df_list_name + ' ' + i \n",
    "    df_x = df_x.join(j, how='inner', rsuffix='_' + i)\n",
    "\n",
    "#指定の期間を抽出\n",
    "if end_date == '':\n",
    "    df_x = df_x[start_date:]\n",
    "else:\n",
    "    df_x = df_x[start_date:end_date]\n",
    "    \n",
    "print(df_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ラベルデータの用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            jump  rise  fall  drop\n",
      "Date                              \n",
      "2009-03-10   0.0   0.0   0.0   1.0\n",
      "2009-03-11   0.0   1.0   0.0   0.0\n",
      "2009-03-12   0.0   1.0   0.0   0.0\n",
      "2009-03-13   0.0   1.0   0.0   0.0\n",
      "2009-03-16   0.0   1.0   0.0   0.0\n",
      "2009-03-17   0.0   0.0   0.0   1.0\n",
      "2009-03-18   0.0   0.0   0.0   1.0\n",
      "2009-03-19   1.0   0.0   0.0   0.0\n",
      "2009-03-20   1.0   0.0   0.0   0.0\n",
      "2009-03-23   0.0   1.0   0.0   0.0\n",
      "2009-03-24   0.0   0.0   1.0   0.0\n",
      "2009-03-25   1.0   0.0   0.0   0.0\n",
      "2009-03-26   0.0   0.0   1.0   0.0\n",
      "2009-03-27   0.0   0.0   1.0   0.0\n",
      "2009-03-30   1.0   0.0   0.0   0.0\n",
      "2009-03-31   0.0   0.0   1.0   0.0\n",
      "2009-04-01   0.0   1.0   0.0   0.0\n",
      "2009-04-02   0.0   1.0   0.0   0.0\n",
      "2009-04-03   0.0   1.0   0.0   0.0\n",
      "2009-04-06   0.0   0.0   1.0   0.0\n",
      "2009-04-07   0.0   0.0   1.0   0.0\n",
      "2009-04-08   0.0   1.0   0.0   0.0\n",
      "2009-04-09   0.0   0.0   1.0   0.0\n",
      "2009-04-10   0.0   0.0   1.0   0.0\n",
      "2009-04-13   0.0   0.0   0.0   1.0\n",
      "2009-04-14   0.0   1.0   0.0   0.0\n",
      "2009-04-15   0.0   0.0   1.0   0.0\n",
      "2009-04-16   0.0   0.0   1.0   0.0\n",
      "2009-04-17   0.0   0.0   0.0   1.0\n",
      "2009-04-20   0.0   1.0   0.0   0.0\n",
      "...          ...   ...   ...   ...\n",
      "2018-09-19   0.0   1.0   0.0   0.0\n",
      "2018-09-20   0.0   1.0   0.0   0.0\n",
      "2018-09-21   0.0   1.0   0.0   0.0\n",
      "2018-09-24   0.0   1.0   0.0   0.0\n",
      "2018-09-25   0.0   0.0   1.0   0.0\n",
      "2018-09-26   0.0   1.0   0.0   0.0\n",
      "2018-09-27   0.0   1.0   0.0   0.0\n",
      "2018-09-28   0.0   1.0   0.0   0.0\n",
      "2018-10-01   0.0   0.0   1.0   0.0\n",
      "2018-10-02   0.0   1.0   0.0   0.0\n",
      "2018-10-03   0.0   0.0   1.0   0.0\n",
      "2018-10-04   0.0   0.0   1.0   0.0\n",
      "2018-10-05   0.0   0.0   1.0   0.0\n",
      "2018-10-08   0.0   0.0   1.0   0.0\n",
      "2018-10-09   0.0   0.0   1.0   0.0\n",
      "2018-10-10   0.0   0.0   1.0   0.0\n",
      "2018-10-11   0.0   1.0   0.0   0.0\n",
      "2018-10-12   0.0   0.0   1.0   0.0\n",
      "2018-10-15   0.0   1.0   0.0   0.0\n",
      "2018-10-16   0.0   1.0   0.0   0.0\n",
      "2018-10-17   0.0   0.0   1.0   0.0\n",
      "2018-10-18   0.0   1.0   0.0   0.0\n",
      "2018-10-19   0.0   1.0   0.0   0.0\n",
      "2018-10-22   0.0   0.0   1.0   0.0\n",
      "2018-10-23   0.0   0.0   1.0   0.0\n",
      "2018-10-24   0.0   1.0   0.0   0.0\n",
      "2018-10-25   0.0   0.0   1.0   0.0\n",
      "2018-10-26   0.0   1.0   0.0   0.0\n",
      "2018-10-29   0.0   1.0   0.0   0.0\n",
      "2018-10-30   0.0   0.0   1.0   0.0\n",
      "\n",
      "[2508 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# ラベルデータを作成する列\n",
    "target = df_x[target_name]\n",
    "#●デバッグ\n",
    "            \n",
    "# 空のデータフレーム作成\n",
    "df_t = pd.DataFrame(index=df_x.index, columns=['jump', 'rise', 'fall', 'drop'])\n",
    "df_t = df_t.fillna(0) #０で埋める\n",
    "\n",
    "# 条件にあった値を置換する\n",
    "df_t.loc[0.995033085 <= target, 'jump'] = 1\n",
    "df_t.loc[(0 <= target) & (target < 0.995033085), 'rise'] = 1\n",
    "df_t.loc[(-0.995033085 <= target) & (target < 0), 'fall'] = 1\n",
    "df_t.loc[target < -0.995033085, 'drop'] = 1\n",
    "\n",
    "# データを整える\n",
    "df_t = df_t.shift(-1 * x_days_later, axis=0)#予測先日数分だけ縦にずらす\n",
    "df_t = df_t.drop(df_t.index[-1*x_days_later:], axis=0)#ラベルデータ末尾の行を削除\n",
    "df_x = df_x.drop(df_x.index[-1*x_days_later:], axis=0)#学習データの末尾の行を削除\n",
    "\n",
    "print(df_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 独立したプロセス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning(q, maxlen, n_hidden, n_in, n_out, learning_model, x_train, t_train, batch_size, epochs):\n",
    "    # モデル定義\n",
    "    prediction = Prediction(maxlen, n_hidden, n_in, n_out, learning_model)\n",
    "    # 学習\n",
    "    model = prediction.train(x_train, t_train, batch_size, epochs)\n",
    "    # データを送信\n",
    "    q.put(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メイン処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_seq 2409 \n",
      "\n",
      "x.shape= (2409, 100, 5)\n",
      "t.shape= (2409, 4) \n",
      "\n",
      "x_train.shape= (2168, 100, 5)\n",
      "x_test.shape= (241, 100, 5) \n",
      "\n",
      "t_train.shape= (2168, 4)\n",
      "t_test.shape= (241, 4) \n",
      "\n",
      "maxlen:  100\n",
      "n_hidden:  100 \n",
      "\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-cfa4fe663373>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQueue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "# 結果を記録する変数\n",
    "result_csv = pd.DataFrame(columns=['day', 'model', 'start date', 'end date', 'maxlen', 'n_hidden', 'combination', 'time', 'correct', 'semi_correct'])\n",
    "debug_csv = pd.DataFrame(columns=['time', 'x_train.shape', 't_train.shape', 'batch size', 'epochs'])\n",
    "\n",
    "# インデックスと列名を外す\n",
    "x_data = df_x.values\n",
    "t_data = df_t.values\n",
    "\n",
    "maxlen = min_maxlen # 最小入力系列数\n",
    "while maxlen <= max_maxlen:\n",
    "    n_in = x_data.shape[1]   # 学習データ（＝入力）の列数\n",
    "    n_out = t_data.shape[1]  # ラベルデータ（=出力）の列数\n",
    "    len_seq = x_data.shape[0] - maxlen + 1\n",
    "    print('len_seq', len_seq, '\\n')#●デバッグ\n",
    "    data = []\n",
    "    target = []\n",
    "\n",
    "    #\n",
    "    for i in range(0, len_seq):\n",
    "        data.append(x_data[i:i+maxlen, :])\n",
    "        target.append(t_data[i+maxlen-1, :])\n",
    "\n",
    "    #\n",
    "    x = np.array(data).reshape(len(data), maxlen, n_in)\n",
    "    t = np.array(target).reshape(len(data), n_out)\n",
    "\n",
    "    #\n",
    "    if is_debug == True:\n",
    "        print('x.shape=', x.shape)\n",
    "        print('t.shape=', t.shape, '\\n')\n",
    "\n",
    "    # ここからソースコードの後半\n",
    "    n_train = int(len(data)*0.9)              # 訓練データ長\n",
    "    x_train,x_test = np.vsplit(x, [n_train])  # 学習データを訓練用とテスト用に分割\n",
    "    t_train,t_test = np.vsplit(t, [n_train])  # ラベルデータを訓練用とテスト用に分割\n",
    "\n",
    "    #\n",
    "    if is_debug == True:\n",
    "        print('x_train.shape=', x_train.shape)\n",
    "        print('x_test.shape=', x_test.shape, '\\n')\n",
    "        print('t_train.shape=', t_train.shape)\n",
    "        print('t_test.shape=', t_test.shape, '\\n')\n",
    "            \n",
    "    #メイン処理--------------------------------------------------------------\n",
    "    n_hidden = min_n_hidden # 最小隠れ層ユニット数\n",
    "    while n_hidden <= max_n_hidden:   \n",
    "        epochs = 100      # エポック数（同じデータでの学習回数）\n",
    "        batch_size = 256  #バッチサイズ\n",
    "                \n",
    "        #パラメータの表示\n",
    "        print('maxlen: ', maxlen)\n",
    "        print('n_hidden: ', n_hidden, '\\n')\n",
    "        \n",
    "        # 学習時間の計測開始\n",
    "        start = time.time()\n",
    "        \n",
    "        # モデル定義\n",
    "        #prediction = Prediction(maxlen, n_hidden, n_in, n_out, learning_model)\n",
    "                    \n",
    "        # 学習\n",
    "        #model = prediction.train(x_train, t_train, batch_size, epochs)\n",
    "            \n",
    "        # 学習の実行\n",
    "        q = Queue()\n",
    "        p = Process(target=learning, args=(q, maxlen, n_hidden, n_in, n_out, learning_model, x_train, t_train, batch_size, epochs,))\n",
    "        p.start()\n",
    "        model = q.get()\n",
    "        p.join()\n",
    "            \n",
    "        # 学習時間の計測終了と表示\n",
    "        end = time.time()\n",
    "\n",
    "        #予測精度の評価------------------------------------------------------\n",
    "\n",
    "        # 正答率、準正答率（騰落）集計\n",
    "        preds = model.predict(x_test)\n",
    "                \n",
    "        #正解数を数える変数\n",
    "        correct = 0\n",
    "        semi_correct = 0\n",
    "                \n",
    "        #表を作るためのデータフレーム\n",
    "        matrix = pd.DataFrame(columns=['jump!', 'rise!', 'fall!', 'drop!'], index=['jump?', 'rise?', 'fall?', 'drop?'])\n",
    "        matrix = matrix.fillna(0)\n",
    "\n",
    "        #正解数を数える\n",
    "        for i in range(len(preds)):\n",
    "            pred = np.argmax(preds[i,:])#argmaxとは配列の最大要素のインデックスを返すメソッドである\n",
    "            tar = np.argmax(t_test[i,:])\n",
    "            matrix.iat[pred, tar] = matrix.iat[pred, tar] + 1 #●マトリックスのセルをインクリメント\n",
    "            if pred == tar :#完全一致\n",
    "               correct += 1\n",
    "            else :\n",
    "                if pred+tar == 1 or pred+tar == 5 :\n",
    "                    semi_correct += 1\n",
    "                \n",
    "        #正答率と準正答率を求める\n",
    "        correct_rate = 1.0 *correct / len(preds) \n",
    "        semi_correct_rate = 1.0 * (correct+semi_correct) / len(preds)\n",
    "                \n",
    "        #csvに記録\n",
    "        series = pd.Series([x_days_later,\n",
    "                            learning_model,\n",
    "                            start_date,\n",
    "                            end_date,\n",
    "                            maxlen,\n",
    "                            n_hidden,\n",
    "                            df_list_name,\n",
    "                            int(end - start),\n",
    "                            correct_rate,\n",
    "                            semi_correct_rate],\n",
    "                            index = result_csv.columns)\n",
    "        result_csv = result_csv.append(series, ignore_index = True)\n",
    "        result_csv.to_csv('./log_' + learning_model + '/' + df_list_name + '.csv', index=False)\n",
    "        \n",
    "        #csvに記録\n",
    "        series2 = pd.Series([int(end - start),\n",
    "                             x_train.shape,\n",
    "                             t_train.shape,\n",
    "                             batch_size,\n",
    "                             epochs],\n",
    "                             index = debug_csv.columns)\n",
    "        debug_csv = debug_csv.append(series2, ignore_index = True)\n",
    "        debug_csv.to_csv('./log_' + learning_model + '/debug.csv', index=False)\n",
    "                \n",
    "        #表と正答率と学習時間を表示\n",
    "        print('\\n', matrix, '\\n')\n",
    "        print(\"正答率:\", 1.0 * correct / len(preds))\n",
    "        print(\"準正答率（騰落）:\", 1.0 * (correct+semi_correct) / len(preds))                \n",
    "                \n",
    "        #次のステップへ\n",
    "        n_hidden += 100\n",
    "                    \n",
    "    #次のステップへ\n",
    "    maxlen += 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 終了の合図"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    winsound.Beep(784,300)\n",
    "    winsound.Beep(698,300)\n",
    "    winsound.Beep(784,600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最高正答率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('平均計算時間')\n",
    "print(result_csv['time'].mean(), '秒')\n",
    "    \n",
    "print('max correct')\n",
    "print(result_csv[result_csv['correct'] == result_csv['correct'].max()])\n",
    "    \n",
    "print('\\nmax semi correct')\n",
    "print(result_csv[result_csv['semi_correct'] == result_csv['semi_correct'].max()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平均正答率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('正答率')\n",
    "print(result_csv['correct'].mean())\n",
    "\n",
    "print('準正答率')\n",
    "print(result_csv['semi_correct'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果を記録"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_csv = pd.DataFrame(columns=['mean time', 'maxlen', 'n_hidden', 'max correct', 'max semi correct', 'mean correct', 'mean semi correct', 'combination'])\n",
    "\n",
    "#result_csv = pd.read_csv('log/USD_JPY_diff nikkei.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "series3 = pd.Series([result_csv['time'].sum() / 60,\n",
    "                     result_csv['maxlen'][result_csv['semi_correct'].idxmax()],\n",
    "                     result_csv['n_hidden'][result_csv['semi_correct'].idxmax()],\n",
    "                     result_csv['correct'].max(),\n",
    "                     result_csv['semi_correct'].max(),\n",
    "                     result_csv['correct'].mean(),\n",
    "                     result_csv['semi_correct'].mean(),\n",
    "                     df_list_name],\n",
    "                        index = report_csv.columns)\n",
    "\n",
    "report_csv = report_csv.append(series3, ignore_index = True)\n",
    "\n",
    "print(report_csv)\n",
    "\n",
    "if os.path.exists('log_' + learning_model + '/log.csv'):\n",
    "    report_csv.to_csv('log_' + learning_model + '/log.csv', index=False, mode='a', header=False)\n",
    "else:\n",
    "    report_csv.to_csv('log_' + learning_model + '/log.csv', index=False, mode='a', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
